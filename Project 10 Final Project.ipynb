{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graham Bachman - Final Project \n",
    "## Women's E-Commerce Clothing Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting Rating and Recommended IND using Review Text\n",
    "\n",
    "Data source: https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews/\n",
    "\n",
    "+ **Clothing ID**: Unique ID of the product\n",
    "+ **Age**:Age of the reviewer\n",
    "+ **Title**:Title of the review\n",
    "+ **ReviewText**: review\n",
    "+ **Rating**: Product rating by reviewer\n",
    "+ **Recommended IND**: Whether the product is recommended or not by the reviewer\n",
    "+ **Positive Feedback Count**: Number of positive feedback on the review\n",
    "+ **Division Name**: Name of the division product is in\n",
    "+ **Department Name**: Name of the department product is in\n",
    "+ **Class Name**: Type of product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5ff436ebd05f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_sm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "# write your Python codes here for Q 1 and run this cell to get the outputs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import NMF\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "df = pd.read_csv('../Homework/Womens Clothing E-Commerce Reviews.csv', sep=',')\n",
    "print(\"Removed columns that won't be used in the analysis.\")\n",
    "df = df.drop(['Clothing ID', 'Age', 'Title', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for null values\n",
      "Review Text        845\n",
      "Rating               0\n",
      "Recommended IND      0\n",
      "dtype: int64\n",
      "\n",
      "Check how many entries in the dataset.\n",
      "0.036\n",
      "3.7 is a small percentage of the total entries, so I would like to remove the null entries outright\n",
      "\n",
      "Doube check the balance of distribution in the rows I will be removing.\n",
      "5    591\n",
      "4    169\n",
      "3     48\n",
      "1     21\n",
      "2     16\n",
      "Name: Rating, dtype: int64\n",
      "1    774\n",
      "0     71\n",
      "Name: Recommended IND, dtype: int64\n",
      "Seems like the entries I will be removing are not behaving differently than the rest of the data, so safe to drop them\n",
      "\n",
      "Review Text        0\n",
      "Rating             0\n",
      "Recommended IND    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Check for null values')\n",
    "print(df.isnull().sum())\n",
    "print()\n",
    "print('Check how many entries in the dataset.')\n",
    "print(round(845/len(df),3))\n",
    "print('3.7 is a small percentage of the total entries, so I would like to remove the null entries outright')\n",
    "print()\n",
    "print('Doube check the balance of distribution in the rows I will be removing.')\n",
    "df1 = df[df.isna().any(axis=1)]\n",
    "print(df1['Rating'].value_counts())\n",
    "print(df1['Recommended IND'].value_counts())\n",
    "print('Seems like the entries I will be removing are not behaving differently than the rest of the data, so safe to drop them')\n",
    "df = df.dropna()\n",
    "print()\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (18112,)\n",
      "Testing Data Shape:  (4529,)\n"
     ]
    }
   ],
   "source": [
    "X = df['Review Text']\n",
    "y = df['Recommended IND']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y,test_size=0.20, random_state=2020)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_train_tfidf.shape\n",
    "\n",
    "print('Training Data Shape:', X_train.shape)\n",
    "print('Testing Data Shape: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Recommended IND using Review Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['a', 'about', 'an', 'and', 'are', 'as', 'at', 'be', 'been', 'but', 'by', 'can', \\\n",
    "             'even', 'ever', 'for', 'from', 'get', 'had', 'has', 'have', 'he', 'her', 'hers', 'his', \\\n",
    "             'how', 'i', 'if', 'in', 'into', 'is', 'it', 'its', 'just', 'me', 'my', 'of', 'on', 'or', \\\n",
    "             'see', 'seen', 'she', 'so', 'than', 'that', 'the', 'their', 'there', 'they', 'this', \\\n",
    "             'to', 'was', 'we', 'were', 'what', 'when', 'which', 'who', 'will', 'with', 'you']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "text_lr = Pipeline([('tfidf', TfidfVectorizer(stop_words=stopwords)), \n",
    "                     ('lr', LogisticRegression()), \n",
    "])\n",
    "\n",
    "text_lr.fit(X_train, y_train) \n",
    "print('Model Accuracy: ', metrics.accuracy_score(y_test,text_lr.predict(X_test)))\n",
    "print('F1 Score: ', metrics.f1_score(y_test,text_lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "text_nb = Pipeline([('tfidf', TfidfVectorizer(stop_words=stopwords)), \n",
    "                     ('nb', MultinomialNB()), \n",
    "])\n",
    "\n",
    "text_nb.fit(X_train, y_train) \n",
    "print('Model Accuracy: ', metrics.accuracy_score(y_test,text_nb.predict(X_test)))\n",
    "print('F1 Score: ', metrics.f1_score(y_test,text_nb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVC\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer(stop_words=stopwords)), \n",
    "                     ('clf', LinearSVC()), \n",
    "])\n",
    "\n",
    "text_clf.fit(X_train, y_train)  \n",
    "print('Model Accuracy: ', metrics.accuracy_score(y_test,text_clf.predict(X_test)))\n",
    "print('F1 Score: ', metrics.f1_score(y_test,text_clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "text_rf = Pipeline([('tfidf', TfidfVectorizer(stop_words=stopwords)), \n",
    "                     ('rf', RandomForestClassifier()), \n",
    "])\n",
    "\n",
    "text_rf.fit(X_train, y_train) \n",
    "print('Model Accuracy: ', metrics.accuracy_score(y_test,text_rf.predict(X_test)))\n",
    "print('F1 Score: ', metrics.f1_score(y_test,text_rf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The best model is the Logistic Regression, it had the highest accuracy at 0.8907 and the best F1 Score at 0.902.\\nLogistic Regressgion is my recommendation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Rating using Review Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = df['Rating']\n",
    "\n",
    "X_train, X_test, y1_train, y1_test = train_test_split(X, y1, stratify = y1,test_size=0.20, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-072eed2379c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Logistic Regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m text_lr_1 = Pipeline([('tfidf', TfidfVectorizer()), \n\u001b[0m\u001b[0;32m      3\u001b[0m                      \u001b[1;33m(\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m ])\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "text_lr_1 = Pipeline([('tfidf', TfidfVectorizer()), \n",
    "                     ('lr', LogisticRegression()), \n",
    "])\n",
    "\n",
    "text_lr_1.fit(X_train, y1_train) \n",
    "print('Model Accuracy: ', metrics.accuracy_score(y1_test,text_lr_1.predict(X_test)))\n",
    "#print(metrics.f1_score(y1_test,text_lr_1.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "text_nb_1 = Pipeline([('tfidf', TfidfVectorizer(stop_words=stopwords)), \n",
    "                     ('nb', MultinomialNB()), \n",
    "])\n",
    "\n",
    "text_nb_1.fit(X_train, y1_train) \n",
    "print('Model Accuracy: ', metrics.accuracy_score(y1_test,text_nb_1.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVC\n",
    "text_clf_1 = Pipeline([('tfidf', TfidfVectorizer(stop_words=stopwords)), \n",
    "                     ('clf', LinearSVC()), \n",
    "])\n",
    "\n",
    "text_clf_1.fit(X_train, y1_train)  \n",
    "print('Model Accuracy: ', metrics.accuracy_score(y1_test,text_clf_1.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "text_rf_1 = Pipeline([('tfidf', TfidfVectorizer(stop_words=stopwords)), \n",
    "                     ('rf', RandomForestClassifier()), \n",
    "])\n",
    "\n",
    "text_rf_1.fit(X_train, y1_train) \n",
    "print('Model Accuracy: ', metrics.accuracy_score(y1_test,text_rf_1.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The best model is the Logistic Regression, it had the highest accuracy at 0.63568.\\nLogistic Regressgion is my recommendation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Stop Words from the Review Text Column\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "df['Review Text'] = df['Review Text'].str.lower().str.split()\n",
    "df['Review Text'] = df['Review Text'].apply(lambda x: [item for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review Text'] = df['Review Text'].to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Review Text'] = df['Review Text'].applymap(str)\n",
    "df['Review Text'] = ' '.join(map(str, df['Review Text'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_df=0.90, min_df=3)\n",
    "dtm = cv.fit_transform(df['Review Text'])\n",
    "LDA = LatentDirichletAllocation(n_components=10,random_state=100)\n",
    "LDA.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(**cv.get_params())\n",
    "dtm_tfidf = tfidf_vectorizer.fit_transform(df['Review Text'])\n",
    "LDA_tfidf = LatentDirichletAllocation(n_components=10, random_state=0)\n",
    "LDA_tfidf.fit(dtm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(LDA, dtm, cv, mds='mmds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(LDA_tfidf, dtm_tfidf, tfidf_vectorizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
